{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모듈 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import easydict\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tf_pose.estimator import TfPoseEstimator\n",
    "from tf_pose.networks import get_graph_path, model_wh\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV(HOG) + tf_Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 import\n",
    "e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(432, 368))\n",
    "body_cascade = cv2.CascadeClassifier(\n",
    "                './opencv-master/opencv-master/data/haarcascades/haarcascade_fullbody.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "count = 0\n",
    "while True:\n",
    "    ret, image = cap.read()\n",
    "    people_body = []\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Skeleton 그리기 with Background\n",
    "    humans = e.inference(image, upsample_size=4.0)\n",
    "    image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "    \n",
    "    # Bounding Box 그리기\n",
    "    body = body_cascade.detectMultiScale(image, 1.01, 10)\n",
    "    for (x,y,w,h) in body:\n",
    "        cv2.rectangle(image,(x,y),(x+w,y+h),(0,0,255),3)\n",
    "        people_body.append(image[int(y):int(y+h), int(x):int(x+w)])\n",
    "    \n",
    "    if cv2.waitKey(10) == 27:                    \n",
    "        break\n",
    "    print('%d.jpg done' % count)\n",
    "    count += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"Gesture_Recognition\", image)\n",
    "\n",
    "#윈도우 종료\n",
    "cap.release()\n",
    "cv2.destroyWindow('Gesture_Recognition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV(cascade) + tf_Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-01-27 23:47:48,442] [TfPoseEstimator] [INFO] loading graph from C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\models\\graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.jpg done\n",
      "1.jpg done\n",
      "2.jpg done\n",
      "3.jpg done\n",
      "4.jpg done\n",
      "5.jpg done\n",
      "6.jpg done\n",
      "7.jpg done\n",
      "8.jpg done\n",
      "9.jpg done\n",
      "10.jpg done\n",
      "11.jpg done\n",
      "12.jpg done\n",
      "13.jpg done\n",
      "14.jpg done\n",
      "15.jpg done\n",
      "16.jpg done\n",
      "17.jpg done\n",
      "18.jpg done\n",
      "19.jpg done\n",
      "20.jpg done\n",
      "21.jpg done\n",
      "22.jpg done\n",
      "23.jpg done\n",
      "24.jpg done\n",
      "25.jpg done\n",
      "26.jpg done\n",
      "27.jpg done\n",
      "28.jpg done\n",
      "29.jpg done\n",
      "30.jpg done\n",
      "31.jpg done\n",
      "32.jpg done\n",
      "33.jpg done\n",
      "34.jpg done\n",
      "35.jpg done\n",
      "36.jpg done\n",
      "37.jpg done\n",
      "38.jpg done\n",
      "39.jpg done\n",
      "40.jpg done\n",
      "41.jpg done\n",
      "42.jpg done\n",
      "43.jpg done\n",
      "44.jpg done\n",
      "45.jpg done\n"
     ]
    }
   ],
   "source": [
    "# 모델 import\n",
    "e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(432, 368))\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "count = 0\n",
    "while True:\n",
    "    ret, image = cap.read()\n",
    "    people_body = []\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Skeleton 그리기 with Background\n",
    "    humans = e.inference(image, upsample_size=4.0)\n",
    "    image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "    \n",
    "    # Bounding Box 그리기\n",
    "    detected, _ = hog.detectMultiScale(image)\n",
    "    for (x,y,w,h) in detected:\n",
    "        cv2.rectangle(image,(x,y),(x+w,y+h),(0,0,255),3)\n",
    "        people_body.append(image[int(y):int(y+h), int(x):int(x+w)])\n",
    "    \n",
    "    if cv2.waitKey(10) == 27:                    \n",
    "        break\n",
    "    print('%d.jpg done' % count)\n",
    "    count += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"Gesture_Recognition\", image)\n",
    "\n",
    "#윈도우 종료\n",
    "cap.release()\n",
    "cv2.destroyWindow('Gesture_Recognition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV Test with haarcascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-01-27 23:34:09,521] [TfPoseEstimator] [INFO] loading graph from C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\models\\graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n"
     ]
    }
   ],
   "source": [
    "# 모델 import\n",
    "e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(432, 368))\n",
    "full_body_cascade = cv2.CascadeClassifier(\n",
    "                './opencv-master/opencv-master/data/haarcascades/haarcascade_fullbody.xml')\n",
    "lower_body_cascade = cv2.CascadeClassifier(\n",
    "                './opencv-master/opencv-master/data/haarcascades/haarcascade_lowerbody.xml')\n",
    "upper_body_cascade = cv2.CascadeClassifier(\n",
    "                './opencv-master/opencv-master/data/haarcascades/haarcascade_upperbody.xml')\n",
    "\n",
    "people_body = []\n",
    "image = cv2.imread('Test.jpg', cv2.IMREAD_COLOR)\n",
    "\n",
    "\n",
    "# Skeleton 그리기 with Background\n",
    "humans = e.inference(image, upsample_size=4.0)\n",
    "image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "\n",
    "# Bounding Box 그리기\n",
    "full_body = full_body_cascade.detectMultiScale(image, 1.01, 10)\n",
    "lower_body = lower_body_cascade.detectMultiScale(image, 1.01, 10)\n",
    "upper_body = upper_body_cascade.detectMultiScale(image, 1.01, 10)\n",
    "\n",
    "for (x,y,w,h) in full_body:\n",
    "    cv2.rectangle(image,(x,y),(x+w,y+h),(0,0,255),3) # 빨간색\n",
    "\n",
    "for (x,y,w,h) in lower_body:\n",
    "    cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),3) # 초록색\n",
    "\n",
    "for (x,y,w,h) in upper_body:\n",
    "    cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),3) # 파란색\n",
    "\n",
    "\n",
    "# if cv2.waitKey(10) == 27:                    \n",
    "#     break\n",
    "# print('%d.jpg done' % count)\n",
    "# count += 1\n",
    "\n",
    "\n",
    "\n",
    "cv2.imshow(\"Gesture_Recognition\", image)\n",
    "cv2.imwrite(\"Test_result_lower_upper.jpg\", image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# #윈도우 종료\n",
    "# cap.release()\n",
    "# cv2.destroyWindow('Gesture_Recognition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(people_body[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_body\n",
    "img = Image.fromarray(people_body[0], 'RGB')\n",
    "img.show()\n",
    "# cv2.imshow(\"Test\", img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV Test with HOG(hostogram of Oriented Gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-01-27 23:42:21,330] [TfPoseEstimator] [INFO] loading graph from C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\models\\graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n"
     ]
    }
   ],
   "source": [
    "# 모델 import\n",
    "e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(432, 368))\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "people_body = []\n",
    "image = cv2.imread('Test.jpg', cv2.IMREAD_COLOR)\n",
    "\n",
    "\n",
    "# Skeleton 그리기 with Background\n",
    "humans = e.inference(image, upsample_size=4.0)\n",
    "image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "\n",
    "# Bounding Box 그리기\n",
    "detected, _ = hog.detectMultiScale(image)\n",
    "\n",
    "for (x, y, w, h) in detected:\n",
    "    cv2.rectangle(image, (x,y),(x+w,y+h), (255,0,0), 3) # 파란색\n",
    "\n",
    "cv2.imshow(\"Gesture_Recognition\", image)\n",
    "# cv2.imwrite(\"Test_result_hog.jpg\", image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# #윈도우 종료\n",
    "# cap.release()\n",
    "# cv2.destroyWindow('Gesture_Recognition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-01-19 16:55:31,874] [TfPoseEstimator] [INFO] loading graph from C:\\Users\\nakhe\\Desktop\\UGRP\\2021_UGRP\\personal\\nakon_zoe\\3. Side Project\\2021.01.25. Gesture Recognition\\2021.01.19. Final_Step\\models\\graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n"
     ]
    }
   ],
   "source": [
    "# 모델 import\n",
    "e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(432, 368))\n",
    "# Dataset Path 지정\n",
    "data_path = ['./1. Original_Datasets/', './2. Jointed_Datasets/', './2. Jointed_Datasets_without_background/']\n",
    "pose_classes = ['1. Pedestrian/', '2. Lier/', '3. Sitter/', '4. Taxier/', '5. Blocker/']\n",
    "\n",
    "    \n",
    "for pose_class in pose_classes:\n",
    "\n",
    "    original_path = data_path[0]+pose_class\n",
    "    new_path = data_path[1]+pose_class\n",
    "    new_path_without_background = data_path[2]+pose_class\n",
    "    file_list = os.listdir(original_path) # 해당 path 내의 모든 파일 list 가져오기\n",
    "\n",
    "    for file in file_list:\n",
    "        # input 단위 설정\n",
    "        args = easydict.EasyDict({\"image\": original_path+file})\n",
    "\n",
    "        # image 로드\n",
    "        image = cv2.imread(args.image)\n",
    "        \n",
    "        # Skeleton 그리기 with Background\n",
    "        humans = e.inference(image, upsample_size=4.0)\n",
    "        image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "        \n",
    "        # 이미지 저장\n",
    "        cv2.imwrite(new_path+file,image)\n",
    "\n",
    "        # Skeleton 그리기 without Background\n",
    "        humans = e.inference(image, upsample_size=4.0)\n",
    "        image = np.zeros(image.shape,dtype=np.uint8)\n",
    "        image.fill(255) \n",
    "        image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "\n",
    "        # 이미지 저장\n",
    "        cv2.imwrite(new_path_without_background+file,image)\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "        \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 특정 이미지 스켈레톤 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tf_pose.estimator import TfPoseEstimator\n",
    "from tf_pose.networks import get_graph_path, model_wh\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-01-19 22:45:29,702] [TfPoseEstimator] [INFO] loading graph from C:\\Users\\nakhe\\Desktop\\UGRP\\2021_UGRP\\personal\\nakon_zoe\\3. Side Project\\2021.01.25. Gesture Recognition\\2021.01.19. Final_Step\\models\\graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 import\n",
    "e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(432, 368))\n",
    "\n",
    "# input 단위 설정\n",
    "args = easydict.EasyDict({\"image\": 'taxier_test.jpg'})\n",
    "\n",
    "# image 로드\n",
    "image = cv2.imread(args.image)\n",
    "\n",
    "# Skeleton 그리기 with Background\n",
    "humans = e.inference(image, upsample_size=4.0)\n",
    "image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "\n",
    "# 이미지 저장\n",
    "cv2.imwrite('taxier_test_back.jpg',image)\n",
    "\n",
    "# Skeleton 그리기 without Background\n",
    "humans = e.inference(image, upsample_size=4.0)\n",
    "image = np.zeros(image.shape,dtype=np.uint8)\n",
    "image.fill(255) \n",
    "image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "\n",
    "# 이미지 저장\n",
    "cv2.imwrite('taxier_test_noback.jpg',image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
