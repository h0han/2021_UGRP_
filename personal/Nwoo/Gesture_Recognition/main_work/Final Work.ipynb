{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import easydict\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tf_pose.estimator import TfPoseEstimator\n",
    "from tf_pose.networks import get_graph_path, model_wh\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from PIL import ImageFont, ImageDraw, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"CNN_best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-01-29 02:27:46,955] [TfPoseEstimator] [INFO] loading graph from C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\models\\graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\tf_pose\\estimator.py:308: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\tf_pose\\estimator.py:309: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\tf_pose\\estimator.py:312: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\tf_pose\\estimator.py:314: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\tf_pose\\estimator.py:325: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\tf_pose\\estimator.py:326: The name tf.image.resize_area is deprecated. Please use tf.compat.v1.image.resize_area instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\tf_pose\\tensblur\\smoother.py:92: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\tf_pose\\estimator.py:335: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\tf_pose\\estimator.py:340: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\tf_pose\\estimator.py:341: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\tf_pose\\estimator.py:343: The name tf.report_uninitialized_variables is deprecated. Please use tf.compat.v1.report_uninitialized_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-Pose, HOG descriptor 객체 생성 \n",
    "e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(432, 368))\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.jpg done\n",
      "1.jpg done\n",
      "2.jpg done\n",
      "3.jpg done\n",
      "4.jpg done\n",
      "5.jpg done\n",
      "6.jpg done\n",
      "7.jpg done\n",
      "8.jpg done\n",
      "9.jpg done\n",
      "10.jpg done\n",
      "11.jpg done\n",
      "12.jpg done\n",
      "13.jpg done\n",
      "14.jpg done\n",
      "15.jpg done\n",
      "16.jpg done\n",
      "17.jpg done\n",
      "18.jpg done\n",
      "19.jpg done\n",
      "20.jpg done\n",
      "21.jpg done\n",
      "22.jpg done\n",
      "23.jpg done\n",
      "24.jpg done\n",
      "25.jpg done\n",
      "26.jpg done\n",
      "27.jpg done\n",
      "28.jpg done\n",
      "29.jpg done\n",
      "30.jpg done\n",
      "31.jpg done\n",
      "32.jpg done\n",
      "33.jpg done\n",
      "34.jpg done\n",
      "35.jpg done\n",
      "36.jpg done\n",
      "37.jpg done\n",
      "38.jpg done\n",
      "39.jpg done\n",
      "40.jpg done\n",
      "41.jpg done\n",
      "42.jpg done\n",
      "43.jpg done\n",
      "44.jpg done\n",
      "45.jpg done\n",
      "46.jpg done\n",
      "47.jpg done\n",
      "48.jpg done\n",
      "49.jpg done\n",
      "50.jpg done\n",
      "51.jpg done\n",
      "52.jpg done\n",
      "53.jpg done\n",
      "54.jpg done\n",
      "55.jpg done\n",
      "56.jpg done\n",
      "57.jpg done\n",
      "58.jpg done\n",
      "59.jpg done\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "cap = cv2.VideoCapture(1)\n",
    "categories = ['pedestrian', 'sitter', 'taxier']\n",
    "\n",
    "while True:\n",
    "    ret, image = cap.read()\n",
    "    people_bbox = dict()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "    # Bounding Box 그리기\n",
    "    detected, _ = hog.detectMultiScale(image)\n",
    "    \n",
    "    # Skeleton 그리기 with Background\n",
    "    humans = e.inference(image, upsample_size=4.0)\n",
    "    image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "    \n",
    "    for (x,y,w,h) in detected:    \n",
    "        # 가로 200pixel 세로 300pixel 이하 제외\n",
    "#         if w < 100 and h < 200: continue\n",
    "\n",
    "        cv2.rectangle(image,(x,y),(x+w,y+h),(0,0,255),3)\n",
    "        \n",
    "        # image resizing and reshape (Preprocessing)\n",
    "        input_image = image[int(y):int(y+h), int(x):int(x+w)]\n",
    "        img = Image.fromarray(input_image, \"RGB\")\n",
    "#         img = Image.open(input_image)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((500,500))\n",
    "        data = np.asarray(img)\n",
    "        \n",
    "        X = np.array(data)\n",
    "        X = X.astype(\"float\") / 256\n",
    "        X = X.reshape(-1, 500,500,3)\n",
    "        \n",
    "        # model prediction\n",
    "        result = [np.argmax(value) for value in model.predict(X)] \n",
    "        result = categories[result[0]]\n",
    "        people_bbox[(x,y,w,h)] = result\n",
    "        \n",
    "    \n",
    "    # esc 누르면 종료\n",
    "    if cv2.waitKey(10) == 27:                    \n",
    "        break\n",
    "    print('%d.jpg done' % count)\n",
    "    count += 1\n",
    "    \n",
    "    # Class name print \n",
    "    for (x,y,w,h) in people_bbox:\n",
    "        gesture = people_bbox[(x,y,w,h)]\n",
    "        cv2.putText(image, gesture, (x,y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow(\"Gesture_Recognition\", image)\n",
    "#윈도우 종료\n",
    "cap.release()\n",
    "cv2.destroyWindow('Gesture_Recognition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-01-29 00:12:50,285] [TfPoseEstimator] [INFO] loading graph from C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\models\\graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n"
     ]
    }
   ],
   "source": [
    "# 모델 import\n",
    "e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(432, 368))\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "people_bbox = dict()\n",
    "categories = ['pedestrian', 'sitter', 'taxier']\n",
    "image = cv2.imread('Test.jpg', cv2.IMREAD_COLOR)\n",
    "\n",
    "# Bounding Box 그리기\n",
    "detected, _ = hog.detectMultiScale(image)\n",
    "\n",
    "# Skeleton 그리기 with Background\n",
    "humans = e.inference(image, upsample_size=4.0)\n",
    "image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)    \n",
    "\n",
    "for (x, y, w, h) in detected:\n",
    "    if w < 200 and h < 300: continue\n",
    "    cv2.rectangle(image, (x,y),(x+w,y+h), (255,0,0), 3) # 파란색\n",
    "    # image resizing and reshape (Preprocessing)\n",
    "    input_image = image[int(y):int(y+h), int(x):int(x+w)]\n",
    "    img = Image.fromarray(input_image, \"RGB\")\n",
    "#     img = Image.open(input_image)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((500,500))\n",
    "    data = np.asarray(img)\n",
    "\n",
    "    X = np.array(data)\n",
    "    X = X.astype(\"float\") / 256\n",
    "    X = X.reshape(-1, 500,500,3)\n",
    "\n",
    "    # model prediction\n",
    "    pred = model.predict(X)\n",
    "    result = [np.argmax(value) for value in pred] \n",
    "    result = categories[result[0]]\n",
    "    people_bbox[(x,y,w,h)] = result\n",
    "          \n",
    "    \n",
    "# Class name print \n",
    "for (x,y,w,h) in people_bbox:\n",
    "    text = people_bbox[(x,y,w,h)]\n",
    "    cv2.putText(image, text, (x,y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "    \n",
    "cv2.imshow(\"Gesture_Recognition\", image)\n",
    "cv2.imwrite(\"Test_final.jpg\", image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# #윈도우 종료\n",
    "# cap.release()\n",
    "# cv2.destroyWindow('Gesture_Recognition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
