{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모듈 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tf_pose.estimator import TfPoseEstimator\n",
    "from tf_pose.networks import get_graph_path, model_wh\n",
    "import os\n",
    "import time\n",
    "from PIL import ImageFont, ImageDraw, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Detector:OpenCV(HOG) + tf_Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-31-f6fca9eed9e4>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-31-f6fca9eed9e4>\"\u001b[1;36m, line \u001b[1;32m25\u001b[0m\n\u001b[1;33m    model ~~~~\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# TF-Pose, HOG descriptor 객체 생성 \n",
    "e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(432, 368))\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "cap = cv2.VideoCapture(\"./Test_video.mp4\")\n",
    "\n",
    "count = 0\n",
    "while True:\n",
    "    ret, image = cap.read()\n",
    "    people_bbox = dict()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    # Bounding Box 그리기\n",
    "    detected, _ = hog.detectMultiScale(image)\n",
    "    \n",
    "    for (x,y,w,h) in detected:    \n",
    "        # 가로 200pixel 세로 300pixel 이하 제외\n",
    "        if w < 200 and h < 300: continue\n",
    "        \n",
    "        cv2.rectangle(image,(x,y),(x+w,y+h),(0,0,255),3)\n",
    "        \n",
    "        # model prediction\n",
    "        input = image[int(y):int(y+h), int(x):int(x+w)]\n",
    "        model ~~~~\n",
    "        prediction = ~~~\n",
    "\n",
    "        people_bbox[(x,y,w,h)] = prediction\n",
    "        \n",
    "    # Skeleton 그리기 with Background\n",
    "    humans = e.inference(image, upsample_size=4.0)\n",
    "    image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "    \n",
    "    # esc 누르면 종료\n",
    "    if cv2.waitKey(10) == 27:                    \n",
    "        break\n",
    "    print('%d.jpg done' % count)\n",
    "    count += 1\n",
    "    \n",
    "    # Class name print \n",
    "    for (x,y,w,h) in people_bbox:\n",
    "        gesture = people_bbox[(x,y,w,h)]\n",
    "        cv2.putText(image, gesture, (x,y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow(\"Gesture_Recognition\", image)\n",
    "\n",
    "#윈도우 종료\n",
    "cap.release()\n",
    "cv2.destroyWindow('Gesture_Recognition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV(cascade) + tf_Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 import\n",
    "e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(432, 368))\n",
    "body_cascade = cv2.CascadeClassifier(\n",
    "                './opencv-master/opencv-master/data/haarcascades/haarcascade_fullbody.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "count = 0\n",
    "while True:\n",
    "    ret, image = cap.read()\n",
    "    people_body = []\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Skeleton 그리기 with Background\n",
    "    humans = e.inference(image, upsample_size=4.0)\n",
    "    image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "    \n",
    "    # Bounding Box 그리기\n",
    "    body = body_cascade.detectMultiScale(image, 1.01, 10)\n",
    "    for (x,y,w,h) in body:\n",
    "        cv2.rectangle(image,(x,y),(x+w,y+h),(0,0,255),3)\n",
    "        people_body.append(image[int(y):int(y+h), int(x):int(x+w)])\n",
    "    \n",
    "    if cv2.waitKey(10) == 27:                    \n",
    "        break\n",
    "    print('%d.jpg done' % count)\n",
    "    count += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"Gesture_Recognition\", image)\n",
    "\n",
    "#윈도우 종료\n",
    "cap.release()\n",
    "cv2.destroyWindow('Gesture_Recognition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV Test with haarcascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-01-27 23:34:09,521] [TfPoseEstimator] [INFO] loading graph from C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\models\\graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n"
     ]
    }
   ],
   "source": [
    "# 모델 import\n",
    "e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(432, 368))\n",
    "full_body_cascade = cv2.CascadeClassifier(\n",
    "                './opencv-master/opencv-master/data/haarcascades/haarcascade_fullbody.xml')\n",
    "lower_body_cascade = cv2.CascadeClassifier(\n",
    "                './opencv-master/opencv-master/data/haarcascades/haarcascade_lowerbody.xml')\n",
    "upper_body_cascade = cv2.CascadeClassifier(\n",
    "                './opencv-master/opencv-master/data/haarcascades/haarcascade_upperbody.xml')\n",
    "\n",
    "people_body = []\n",
    "image = cv2.imread('Test.jpg', cv2.IMREAD_COLOR)\n",
    "\n",
    "\n",
    "# Skeleton 그리기 with Background\n",
    "humans = e.inference(image, upsample_size=4.0)\n",
    "image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "\n",
    "# Bounding Box 그리기\n",
    "full_body = full_body_cascade.detectMultiScale(image, 1.01, 10)\n",
    "lower_body = lower_body_cascade.detectMultiScale(image, 1.01, 10)\n",
    "upper_body = upper_body_cascade.detectMultiScale(image, 1.01, 10)\n",
    "\n",
    "for (x,y,w,h) in full_body:\n",
    "    cv2.rectangle(image,(x,y),(x+w,y+h),(0,0,255),3) # 빨간색\n",
    "\n",
    "for (x,y,w,h) in lower_body:\n",
    "    cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),3) # 초록색\n",
    "\n",
    "for (x,y,w,h) in upper_body:\n",
    "    cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),3) # 파란색\n",
    "\n",
    "\n",
    "# if cv2.waitKey(10) == 27:                    \n",
    "#     break\n",
    "# print('%d.jpg done' % count)\n",
    "# count += 1\n",
    "\n",
    "\n",
    "\n",
    "cv2.imshow(\"Gesture_Recognition\", image)\n",
    "cv2.imwrite(\"Test_result_lower_upper.jpg\", image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# #윈도우 종료\n",
    "# cap.release()\n",
    "# cv2.destroyWindow('Gesture_Recognition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(people_body[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_body\n",
    "img = Image.fromarray(people_body[0], 'RGB')\n",
    "img.show()\n",
    "# cv2.imshow(\"Test\", img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV Test with HOG(histogram of Oriented Gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-01-28 13:18:14,590] [TfPoseEstimator] [INFO] loading graph from C:\\Users\\pc\\Desktop\\2021_UGRP\\personal\\Nwoo\\Gesture_Recognition\\main_work\\models\\graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(474, 262, 423, 846)]\n"
     ]
    }
   ],
   "source": [
    "# 모델 import\n",
    "e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(432, 368))\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "people_bbox = []\n",
    "people_body = []\n",
    "image = cv2.imread('Test.jpg', cv2.IMREAD_COLOR)\n",
    "\n",
    "# Bounding Box 그리기\n",
    "detected, _ = hog.detectMultiScale(image)\n",
    "for (x, y, w, h) in detected:\n",
    "    if w < 200 and h < 300: continue\n",
    "    cv2.rectangle(image, (x,y),(x+w,y+h), (255,0,0), 3) # 파란색\n",
    "    people_body.append(image[int(y):int(y+h), int(x):int(x+w)])\n",
    "    people_bbox.append((x,y,w,h))\n",
    "    \n",
    "    \n",
    "\n",
    "# # Skeleton 그리기 with Background\n",
    "humans = e.inference(image, upsample_size=4.0)\n",
    "image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)    \n",
    "    \n",
    "# Class name print \n",
    "for (x,y,w,h) in people_bbox:\n",
    "    text = \"test\"\n",
    "    cv2.putText(image, text, (x,y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "    \n",
    "print(people_bbox)\n",
    "    \n",
    "cv2.imshow(\"Gesture_Recognition\", image)\n",
    "# cv2.imwrite(\"Test_result_hog.jpg\", image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# #윈도우 종료\n",
    "# cap.release()\n",
    "# cv2.destroyWindow('Gesture_Recognition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-01-19 16:55:31,874] [TfPoseEstimator] [INFO] loading graph from C:\\Users\\nakhe\\Desktop\\UGRP\\2021_UGRP\\personal\\nakon_zoe\\3. Side Project\\2021.01.25. Gesture Recognition\\2021.01.19. Final_Step\\models\\graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n"
     ]
    }
   ],
   "source": [
    "# 모델 import\n",
    "e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(432, 368))\n",
    "# Dataset Path 지정\n",
    "data_path = ['./1. Original_Datasets/', './2. Jointed_Datasets/', './2. Jointed_Datasets_without_background/']\n",
    "pose_classes = ['1. Pedestrian/', '2. Lier/', '3. Sitter/', '4. Taxier/', '5. Blocker/']\n",
    "\n",
    "    \n",
    "for pose_class in pose_classes:\n",
    "\n",
    "    original_path = data_path[0]+pose_class\n",
    "    new_path = data_path[1]+pose_class\n",
    "    new_path_without_background = data_path[2]+pose_class\n",
    "    file_list = os.listdir(original_path) # 해당 path 내의 모든 파일 list 가져오기\n",
    "\n",
    "    for file in file_list:\n",
    "        # input 단위 설정\n",
    "        args = easydict.EasyDict({\"image\": original_path+file})\n",
    "\n",
    "        # image 로드\n",
    "        image = cv2.imread(args.image)\n",
    "        \n",
    "        # Skeleton 그리기 with Background\n",
    "        humans = e.inference(image, upsample_size=4.0)\n",
    "        image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "        \n",
    "        # 이미지 저장\n",
    "        cv2.imwrite(new_path+file,image)\n",
    "\n",
    "        # Skeleton 그리기 without Background\n",
    "        humans = e.inference(image, upsample_size=4.0)\n",
    "        image = np.zeros(image.shape,dtype=np.uint8)\n",
    "        image.fill(255) \n",
    "        image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "\n",
    "        # 이미지 저장\n",
    "        cv2.imwrite(new_path_without_background+file,image)\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "        \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 특정 이미지 스켈레톤 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tf_pose.estimator import TfPoseEstimator\n",
    "from tf_pose.networks import get_graph_path, model_wh\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-01-19 22:45:29,702] [TfPoseEstimator] [INFO] loading graph from C:\\Users\\nakhe\\Desktop\\UGRP\\2021_UGRP\\personal\\nakon_zoe\\3. Side Project\\2021.01.25. Gesture Recognition\\2021.01.19. Final_Step\\models\\graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 import\n",
    "e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(432, 368))\n",
    "\n",
    "# input 단위 설정\n",
    "args = easydict.EasyDict({\"image\": 'taxier_test.jpg'})\n",
    "\n",
    "# image 로드\n",
    "image = cv2.imread(args.image)\n",
    "\n",
    "# Skeleton 그리기 with Background\n",
    "humans = e.inference(image, upsample_size=4.0)\n",
    "image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "\n",
    "# 이미지 저장\n",
    "cv2.imwrite('taxier_test_back.jpg',image)\n",
    "\n",
    "# Skeleton 그리기 without Background\n",
    "humans = e.inference(image, upsample_size=4.0)\n",
    "image = np.zeros(image.shape,dtype=np.uint8)\n",
    "image.fill(255) \n",
    "image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "\n",
    "# 이미지 저장\n",
    "cv2.imwrite('taxier_test_noback.jpg',image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
